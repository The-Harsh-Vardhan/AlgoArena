{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd4faf6",
   "metadata": {},
   "source": [
    "# üñºÔ∏è Image Data Algorithms - Fashion-MNIST Classification\n",
    "\n",
    "Welcome to the Image Data arena! In this notebook, we'll apply various machine learning algorithms to the **Fashion-MNIST dataset** - a more challenging alternative to the classic MNIST digits dataset.\n",
    "\n",
    "## üìä Dataset: Fashion-MNIST\n",
    "\n",
    "- **Dataset Size**: 60,000 training + 10,000 test images\n",
    "- **Image Size**: 28√ó28 grayscale pixels\n",
    "- **Classes**: 10 fashion categories\n",
    "- **Source**: Zalando Research\n",
    "\n",
    "## ü§ñ Algorithms Implemented:\n",
    "\n",
    "1. **K-Nearest Neighbors (KNN)**\n",
    "2. **Random Forest Classifier**\n",
    "3. **Support Vector Machine (SVM)**\n",
    "4. **Convolutional Neural Network (CNN)**\n",
    "5. **Logistic Regression**\n",
    "\n",
    "## üéØ Goals:\n",
    "\n",
    "- Compare traditional ML vs Deep Learning approaches\n",
    "- Evaluate performance across multiple metrics\n",
    "- Generate comprehensive results for dashboard visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Sklearn available\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Load and Explore Fashion-MNIST Dataset\n",
    "\n",
    "# Load Fashion-MNIST dataset\n",
    "print(\"üîÑ Loading Fashion-MNIST dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Fashion-MNIST class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Dataset information\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Pixel value range: {X_train.min()} - {X_train.max()}\")\n",
    "\n",
    "# Check class distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
    "    print(f\"{class_names[class_idx]}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Data Preprocessing and Normalization\n",
    "\n",
    "print(\"üîÑ Preprocessing data...\")\n",
    "\n",
    "# Normalize pixel values to 0-1 range\n",
    "X_train_normalized = X_train.astype('float32') / 255.0\n",
    "X_test_normalized = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten images for traditional ML algorithms\n",
    "X_train_flat = X_train_normalized.reshape(X_train_normalized.shape[0], -1)\n",
    "X_test_flat = X_test_normalized.reshape(X_test_normalized.shape[0], -1)\n",
    "\n",
    "# Prepare data for CNN (add channel dimension)\n",
    "X_train_cnn = X_train_normalized.reshape(-1, 28, 28, 1)\n",
    "X_test_cnn = X_test_normalized.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Convert labels to categorical for neural networks\n",
    "y_train_categorical = to_categorical(y_train, 10)\n",
    "y_test_categorical = to_categorical(y_test, 10)\n",
    "\n",
    "print(\"‚úÖ Data preprocessing completed!\")\n",
    "print(f\"Flattened training data shape: {X_train_flat.shape}\")\n",
    "print(f\"CNN training data shape: {X_train_cnn.shape}\")\n",
    "print(f\"Normalized pixel range: {X_train_normalized.min():.3f} - {X_train_normalized.max():.3f}\")\n",
    "print(f\"Categorical labels shape: {y_train_categorical.shape}\")\n",
    "\n",
    "# Subset data for faster traditional ML training (optional)\n",
    "subset_size = 10000\n",
    "X_train_subset = X_train_flat[:subset_size]\n",
    "y_train_subset = y_train[:subset_size]\n",
    "print(f\"\\nUsing subset of {subset_size} samples for traditional ML algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize Sample Images and Class Distribution\n",
    "\n",
    "# Plot sample images from each class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "fig.suptitle('Fashion-MNIST Sample Images by Class', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Find first occurrence of each class\n",
    "    class_idx = np.where(y_train == i)[0][0]\n",
    "    \n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    \n",
    "    axes[row, col].imshow(X_train[class_idx], cmap='gray')\n",
    "    axes[row, col].set_title(f'{i}: {class_name}', fontsize=12)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Class distribution visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Bar plot\n",
    "plt.subplot(1, 2, 1)\n",
    "counts = [np.sum(y_train == i) for i in range(10)]\n",
    "bars = plt.bar(range(10), counts, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "plt.xlabel('Class Index')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xticks(range(10), [f'{i}' for i in range(10)])\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 50,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(counts, labels=[class_names[i] for i in range(10)], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Class Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Sample images and distribution visualized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Initialize Results Storage\n",
    "results = {}\n",
    "performance_summary = []\n",
    "\n",
    "print(\"üìã Results storage initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6653f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¢ Implement K-Nearest Neighbors (KNN)\n",
    "\n",
    "print(\"üîÑ Training KNN Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and train KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "knn.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Training time\n",
    "knn_training_time = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn.predict(X_test_flat)\n",
    "\n",
    "# Calculate accuracy\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "knn_conf_matrix = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_knn = cross_val_score(knn, X_train_subset, y_train_subset, cv=3, n_jobs=-1)\n",
    "\n",
    "print(f\"‚úÖ KNN Training completed!\")\n",
    "print(f\"KNN Accuracy: {knn_accuracy:.4f}\")\n",
    "print(f\"KNN Training Time: {knn_training_time:.2f} seconds\")\n",
    "print(f\"KNN CV Score: {cv_scores_knn.mean():.4f} ¬± {cv_scores_knn.std():.4f}\")\n",
    "\n",
    "# Store results\n",
    "results['KNN'] = {\n",
    "    'accuracy': knn_accuracy,\n",
    "    'training_time': knn_training_time,\n",
    "    'confusion_matrix': knn_conf_matrix.tolist(),\n",
    "    'predictions': y_pred_knn.tolist(),\n",
    "    'cv_mean': cv_scores_knn.mean(),\n",
    "    'cv_std': cv_scores_knn.std(),\n",
    "    'model_type': 'traditional_ml'\n",
    "}\n",
    "\n",
    "performance_summary.append({\n",
    "    'Algorithm': 'KNN',\n",
    "    'Accuracy': knn_accuracy,\n",
    "    'Training Time (s)': knn_training_time,\n",
    "    'CV Score': cv_scores_knn.mean()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üå≤ Implement Random Forest Classifier\n",
    "\n",
    "print(\"üîÑ Training Random Forest Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Training time\n",
    "rf_training_time = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test_flat)\n",
    "\n",
    "# Calculate accuracy\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_rf = cross_val_score(rf, X_train_subset, y_train_subset, cv=3, n_jobs=-1)\n",
    "\n",
    "print(f\"‚úÖ Random Forest Training completed!\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Random Forest Training Time: {rf_training_time:.2f} seconds\")\n",
    "print(f\"Random Forest CV Score: {cv_scores_rf.mean():.4f} ¬± {cv_scores_rf.std():.4f}\")\n",
    "\n",
    "# Store results\n",
    "results['Random Forest'] = {\n",
    "    'accuracy': rf_accuracy,\n",
    "    'training_time': rf_training_time,\n",
    "    'confusion_matrix': rf_conf_matrix.tolist(),\n",
    "    'predictions': y_pred_rf.tolist(),\n",
    "    'cv_mean': cv_scores_rf.mean(),\n",
    "    'cv_std': cv_scores_rf.std(),\n",
    "    'model_type': 'traditional_ml'\n",
    "}\n",
    "\n",
    "performance_summary.append({\n",
    "    'Algorithm': 'Random Forest',\n",
    "    'Accuracy': rf_accuracy,\n",
    "    'Training Time (s)': rf_training_time,\n",
    "    'CV Score': cv_scores_rf.mean()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d50fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° Implement Support Vector Machine (SVM)\n",
    "\n",
    "print(\"üîÑ Training SVM Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and train SVM (using smaller subset for SVM due to computational complexity)\n",
    "svm_subset_size = 5000\n",
    "X_train_svm = X_train_flat[:svm_subset_size]\n",
    "y_train_svm = y_train[:svm_subset_size]\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "svm.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "# Training time\n",
    "svm_training_time = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm.predict(X_test_flat)\n",
    "\n",
    "# Calculate accuracy\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "svm_conf_matrix = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Cross-validation (on smaller subset)\n",
    "cv_scores_svm = cross_val_score(svm, X_train_svm, y_train_svm, cv=3)\n",
    "\n",
    "print(f\"‚úÖ SVM Training completed!\")\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(f\"SVM Training Time: {svm_training_time:.2f} seconds\")\n",
    "print(f\"SVM CV Score: {cv_scores_svm.mean():.4f} ¬± {cv_scores_svm.std():.4f}\")\n",
    "print(f\"Note: SVM trained on {svm_subset_size} samples for computational efficiency\")\n",
    "\n",
    "# Store results\n",
    "results['SVM'] = {\n",
    "    'accuracy': svm_accuracy,\n",
    "    'training_time': svm_training_time,\n",
    "    'confusion_matrix': svm_conf_matrix.tolist(),\n",
    "    'predictions': y_pred_svm.tolist(),\n",
    "    'cv_mean': cv_scores_svm.mean(),\n",
    "    'cv_std': cv_scores_svm.std(),\n",
    "    'model_type': 'traditional_ml'\n",
    "}\n",
    "\n",
    "performance_summary.append({\n",
    "    'Algorithm': 'SVM',\n",
    "    'Accuracy': svm_accuracy,\n",
    "    'Training Time (s)': svm_training_time,\n",
    "    'CV Score': cv_scores_svm.mean()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838dccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Implement Logistic Regression\n",
    "\n",
    "print(\"üîÑ Training Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and train Logistic Regression\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1)\n",
    "lr.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Training time\n",
    "lr_training_time = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr.predict(X_test_flat)\n",
    "\n",
    "# Calculate accuracy\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_conf_matrix = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_lr = cross_val_score(lr, X_train_subset, y_train_subset, cv=3, n_jobs=-1)\n",
    "\n",
    "print(f\"‚úÖ Logistic Regression Training completed!\")\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Logistic Regression Training Time: {lr_training_time:.2f} seconds\")\n",
    "print(f\"Logistic Regression CV Score: {cv_scores_lr.mean():.4f} ¬± {cv_scores_lr.std():.4f}\")\n",
    "\n",
    "# Store results\n",
    "results['Logistic Regression'] = {\n",
    "    'accuracy': lr_accuracy,\n",
    "    'training_time': lr_training_time,\n",
    "    'confusion_matrix': lr_conf_matrix.tolist(),\n",
    "    'predictions': y_pred_lr.tolist(),\n",
    "    'cv_mean': cv_scores_lr.mean(),\n",
    "    'cv_std': cv_scores_lr.std(),\n",
    "    'model_type': 'traditional_ml'\n",
    "}\n",
    "\n",
    "performance_summary.append({\n",
    "    'Algorithm': 'Logistic Regression',\n",
    "    'Accuracy': lr_accuracy,\n",
    "    'Training Time (s)': lr_training_time,\n",
    "    'CV Score': cv_scores_lr.mean()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Define Convolutional Neural Network (CNN) Architecture\n",
    "\n",
    "def create_cnn():\n",
    "    \"\"\"\n",
    "    Create a CNN model for Fashion-MNIST classification\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First convolutional block\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = create_cnn()\n",
    "\n",
    "# Display model architecture\n",
    "print(\"üß† CNN Architecture:\")\n",
    "cnn_model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = cnn_model.count_params()\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• Train Convolutional Neural Network (CNN)\n",
    "\n",
    "print(\"üîÑ Training CNN Model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = cnn_model.fit(\n",
    "    X_train_cnn, y_train_categorical,\n",
    "    validation_data=(X_test_cnn, y_test_categorical),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training time\n",
    "cnn_training_time = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_pred_cnn_prob = cnn_model.predict(X_test_cnn)\n",
    "y_pred_cnn = np.argmax(y_pred_cnn_prob, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "cnn_accuracy = accuracy_score(y_test, y_pred_cnn)\n",
    "cnn_conf_matrix = confusion_matrix(y_test, y_pred_cnn)\n",
    "\n",
    "# Get final loss\n",
    "final_loss = min(history.history['val_loss'])\n",
    "\n",
    "print(f\"‚úÖ CNN Training completed!\")\n",
    "print(f\"CNN Accuracy: {cnn_accuracy:.4f}\")\n",
    "print(f\"CNN Training Time: {cnn_training_time:.2f} seconds\")\n",
    "print(f\"CNN Final Validation Loss: {final_loss:.4f}\")\n",
    "\n",
    "# Store results\n",
    "results['CNN'] = {\n",
    "    'accuracy': cnn_accuracy,\n",
    "    'training_time': cnn_training_time,\n",
    "    'confusion_matrix': cnn_conf_matrix.tolist(),\n",
    "    'predictions': y_pred_cnn.tolist(),\n",
    "    'loss': final_loss,\n",
    "    'model_type': 'deep_learning'\n",
    "}\n",
    "\n",
    "performance_summary.append({\n",
    "    'Algorithm': 'CNN',\n",
    "    'Accuracy': cnn_accuracy,\n",
    "    'Training Time (s)': cnn_training_time,\n",
    "    'CV Score': cnn_accuracy  # Using test accuracy as proxy\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Compare Model Performance\n",
    "\n",
    "print(\"üìä Generating Performance Comparison...\")\n",
    "\n",
    "# Create performance DataFrame\n",
    "performance_df = pd.DataFrame(performance_summary)\n",
    "performance_df = performance_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Model Performance Ranking:\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in performance_df.iterrows():\n",
    "    print(f\"{row['Algorithm']:20} | Accuracy: {row['Accuracy']:.4f} | Time: {row['Training Time (s)']:8.2f}s | CV: {row['CV Score']:.4f}\")\n",
    "\n",
    "# Visualize performance comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].barh(performance_df['Algorithm'], performance_df['Accuracy'], color='skyblue')\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for i, v in enumerate(performance_df['Accuracy']):\n",
    "    axes[0].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "# Training time comparison\n",
    "axes[1].barh(performance_df['Algorithm'], performance_df['Training Time (s)'], color='lightcoral')\n",
    "axes[1].set_xlabel('Training Time (seconds)')\n",
    "axes[1].set_title('Training Time Comparison')\n",
    "\n",
    "# Add time values on bars\n",
    "for i, v in enumerate(performance_df['Training Time (s)']):\n",
    "    axes[1].text(v + 0.5, i, f'{v:.1f}s', va='center')\n",
    "\n",
    "# Accuracy vs Time scatter plot\n",
    "axes[2].scatter(performance_df['Training Time (s)'], performance_df['Accuracy'], \n",
    "               s=100, alpha=0.7, c=range(len(performance_df)), cmap='viridis')\n",
    "axes[2].set_xlabel('Training Time (seconds)')\n",
    "axes[2].set_ylabel('Accuracy')\n",
    "axes[2].set_title('Accuracy vs Training Time Trade-off')\n",
    "\n",
    "# Add algorithm labels\n",
    "for i, row in performance_df.iterrows():\n",
    "    axes[2].annotate(row['Algorithm'], \n",
    "                    (row['Training Time (s)'], row['Accuracy']),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=9, alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best performing algorithm\n",
    "best_algorithm = performance_df.iloc[0]\n",
    "print(f\"\\nü•á Best Performing Algorithm: {best_algorithm['Algorithm']}\")\n",
    "print(f\"   Accuracy: {best_algorithm['Accuracy']:.4f}\")\n",
    "print(f\"   Training Time: {best_algorithm['Training Time (s)']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Generate Confusion Matrices\n",
    "\n",
    "print(\"üîç Generating Confusion Matrices for All Models...\")\n",
    "\n",
    "# Create subplots for confusion matrices\n",
    "n_models = len(results)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, model_results) in enumerate(results.items()):\n",
    "    if idx < len(axes):\n",
    "        cm = np.array(model_results['confusion_matrix'])\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names,\n",
    "                   ax=axes[idx])\n",
    "        axes[idx].set_title(f'Confusion Matrix - {model_name}\\nAccuracy: {model_results[\"accuracy\"]:.4f}')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "        axes[idx].set_ylabel('True Label')\n",
    "        \n",
    "        # Rotate labels for better readability\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "        axes[idx].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Hide unused subplot\n",
    "if len(results) < len(axes):\n",
    "    axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy for best model\n",
    "best_model_name = performance_df.iloc[0]['Algorithm']\n",
    "best_cm = np.array(results[best_model_name]['confusion_matrix'])\n",
    "\n",
    "print(f\"\\nüìà Per-Class Accuracy for Best Model ({best_model_name}):\")\n",
    "print(\"=\" * 50)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_accuracy = best_cm[i, i] / best_cm[i, :].sum()\n",
    "    print(f\"{class_name:15}: {class_accuracy:.4f}\")\n",
    "    \n",
    "# Overall statistics\n",
    "print(f\"\\nOverall Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"Training Time: {results[best_model_name]['training_time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f367e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Save Results to JSON for Dashboard\n",
    "\n",
    "print(\"üíæ Saving results to JSON for Streamlit dashboard...\")\n",
    "\n",
    "# Prepare final results structure\n",
    "final_results = {\n",
    "    'dataset_info': {\n",
    "        'name': 'Fashion-MNIST',\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'num_classes': len(class_names),\n",
    "        'class_names': class_names,\n",
    "        'image_shape': [28, 28, 1]\n",
    "    },\n",
    "    'algorithms': results,\n",
    "    'summary': performance_summary,\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'metadata': {\n",
    "        'notebook_version': '1.0',\n",
    "        'subset_size_traditional_ml': subset_size,\n",
    "        'svm_subset_size': svm_subset_size,\n",
    "        'total_algorithms': len(results)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = '../data/image_results.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved successfully to {output_file}\")\n",
    "print(f\"üìä Total algorithms trained: {len(results)}\")\n",
    "print(f\"üèÜ Best accuracy: {max([r['accuracy'] for r in results.values()]):.4f}\")\n",
    "print(f\"‚ö° Fastest training: {min([r['training_time'] for r in results.values()]):.2f}s\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nüìà Summary Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "accuracies = [r['accuracy'] for r in results.values()]\n",
    "times = [r['training_time'] for r in results.values()]\n",
    "\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\")\n",
    "print(f\"Mean Training Time: {np.mean(times):.2f} ¬± {np.std(times):.2f} seconds\")\n",
    "print(f\"Accuracy Range: {min(accuracies):.4f} - {max(accuracies):.4f}\")\n",
    "print(f\"Time Range: {min(times):.2f}s - {max(times):.2f}s\")\n",
    "\n",
    "print(f\"\\nüéØ Ready for Streamlit Dashboard!\")\n",
    "print(f\"Run: streamlit run streamlit_app/02_Image_Data.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
